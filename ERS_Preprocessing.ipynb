{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ed80e8",
   "metadata": {},
   "source": [
    "### About Dataset\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "1. CREMA-D\n",
    "\n",
    "    The CREMA-D database (Cao et al., 2014) comprises 7,442 video clips of 91 actors (48 male, 43 female) aged 20–74 with diverse ethnicities (African American, Asian, Caucasian, Hispanic, Unspecified), speaking 12 phonetically balanced sentences in six basic emotions (anger, disgust, fear, happy, neutral, sad), each rendered at four intensity levels (Low, Medium, High, Unspecified) in English.\n",
    "\n",
    "#### Filename Annotation\n",
    " \n",
    "* Each of the 7,442 CREMA-D files has a unique filename. The filename consists of a 5-part identifier (e.g., 1001_DFA_ANG_XX_01.mp4). These identifiers define the stimulus characteristics:\n",
    "* Actor ID (1001 to 1091 for each of 91 actors)\n",
    "\n",
    "#### Statement:\n",
    "\n",
    "        IEO = \"It's eleven o'clock\"\n",
    "        TIE = \"That is exactly what happened\"\n",
    "        IOM = \"I'm on my way to the meeting\"\n",
    "        IWW = \"I wonder what this is about\"\n",
    "        TAI = \"The airplane is almost full\"\n",
    "        MTI = \"Maybe tomorrow it will be cold\"\n",
    "        IWL = \"I would like a new alarm clock\"\n",
    "        ITH = \"I think I have a doctor's appointment\"\n",
    "        DFA = \"Don't forget a jacket\"\n",
    "        ITS = \"I think I've seen this before\"\n",
    "        TSI = \"The surface is slick\"\n",
    "        WSI = \"We'll stop in a couple of minutes\"\n",
    "        Emotion (ANG = anger, DIS = disgust, FEA = fear, HAP = happy, NEU = neutral, SAD = sad)\n",
    "        Emotional intensity (LO = low, MD = medium, HI = high, XX = unspecified)\n",
    "        Gender (01 = male, 02 = female)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c501d4c",
   "metadata": {},
   "source": [
    "### Tri-model Preprocessing For CREMA-D Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Trimodal preprocessing (CREMA-D): A/V/P\n",
    "# Audio/Video SAME as your original\n",
    "# Pose UPDATED: MediaPipe 8 body + 70 face + gap-fill\n",
    "# OVERWRITES files in processed_features\n",
    "# ============================================\n",
    "import os, cv2, time, json, shutil, logging, warnings\n",
    "import numpy as np\n",
    "import torch, torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mediapipe as mp\n",
    "from torchvision import models\n",
    "from torchvision.models.efficientnet import EfficientNet_B2_Weights\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# ----------------------------\n",
    "# Logging\n",
    "# ----------------------------\n",
    "LOG_FILE = 'preprocessing.log'\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"facenet_pytorch\")\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "CREMA_D_ROOT = r\"E:\\Research_Datasets\"\n",
    "AUDIO_DIR  = os.path.join(CREMA_D_ROOT, \"CREMA_D_Audio\")\n",
    "VIDEO_DIR  = os.path.join(CREMA_D_ROOT, \"CREMA_D_Videos\")\n",
    "OUTPUT_DIR = os.path.join(CREMA_D_ROOT, \"processed_features2\")\n",
    "TEMP_DIR   = os.path.join(OUTPUT_DIR, \"temp\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_VIDEO_FRAMES = 90   # 3s * 30fps\n",
    "TARGET_AUDIO_FRAMES = 150  # 3s * 50Hz wav2vec\n",
    "\n",
    "EMOTION_MAP = {\"ANG\":0, \"DIS\":1, \"FEA\":2, \"HAP\":3, \"NEU\":4, \"SAD\":5}\n",
    "\n",
    "# ----------------------------\n",
    "# MediaPipe setup (pose + face)\n",
    "# ----------------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_face = mp.solutions.face_mesh\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "face_mesh = mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Pose: 8 body (OpenPose order) + 70 face, with gap-filling\n",
    "# ----------------------------\n",
    "POSE_VIS_THR = 0.5  # require decent visibility for body points\n",
    "\n",
    "def _xy_or_none(lms, idx, min_vis=POSE_VIS_THR):\n",
    "    \"\"\"Return (x,y) from MediaPipe if visible; else None.\"\"\"\n",
    "    if lms is None or idx is None or idx >= len(lms):\n",
    "        return None\n",
    "    pt = lms[idx]\n",
    "    vis = getattr(pt, \"visibility\", 1.0)\n",
    "    if vis < min_vis:\n",
    "        return None\n",
    "    return (pt.x, pt.y)\n",
    "\n",
    "def _mid(a, b):\n",
    "    return None if (a is None or b is None) else ((a[0]+b[0])/2.0, (a[1]+b[1])/2.0)\n",
    "\n",
    "def extract_pose_features(frame):\n",
    "    \"\"\"\n",
    "    156-dim pose vector:\n",
    "      body(8): [nose, neck(mid shoulders), R_sh, L_sh, R_eye, L_eye, R_ear, L_ear]\n",
    "      face(70): first 70 FaceMesh landmarks\n",
    "    Values: normalized (0..1) image coordinates.\n",
    "    \"\"\"\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- body (8 points) ---\n",
    "    body = np.zeros((8, 2), dtype=np.float32)\n",
    "    pr = pose.process(rgb)\n",
    "    pl = pr.pose_landmarks.landmark if (pr and pr.pose_landmarks) else None\n",
    "\n",
    "    nose = _xy_or_none(pl, 0)\n",
    "    LSH  = _xy_or_none(pl, 11)\n",
    "    RSH  = _xy_or_none(pl, 12)\n",
    "    neck = _mid(LSH, RSH)\n",
    "    REYE = _xy_or_none(pl, 5)  # right eye\n",
    "    LEYE = _xy_or_none(pl, 2)  # left eye\n",
    "    REAR = _xy_or_none(pl, 8)  # right ear\n",
    "    LEAR = _xy_or_none(pl, 7)  # left ear\n",
    "\n",
    "    ordered = [nose, neck, RSH, LSH, REYE, LEYE, REAR, LEAR]\n",
    "    for i, p in enumerate(ordered):\n",
    "        if p is not None:\n",
    "            body[i] = p\n",
    "\n",
    "    # --- face (70 points) ---\n",
    "    face = np.zeros((70, 2), dtype=np.float32)\n",
    "    fr = face_mesh.process(rgb)\n",
    "    if fr and fr.multi_face_landmarks:\n",
    "        fl = fr.multi_face_landmarks[0].landmark\n",
    "        n = min(70, len(fl))\n",
    "        for i in range(n):\n",
    "            face[i] = (fl[i].x, fl[i].y)\n",
    "\n",
    "    return np.concatenate([body.reshape(-1), face.reshape(-1)], axis=0)  # (156,)\n",
    "\n",
    "def fill_pose_gaps(pose_arr, max_gap=8):\n",
    "    \"\"\"\n",
    "    pose_arr: (T,156) with zero rows when missing.\n",
    "    - forward/back-fill edges\n",
    "    - linear interpolate internal gaps up to 'max_gap'\n",
    "    - hold-last-sample for longer gaps\n",
    "    Returns gap-filled array of same shape.\n",
    "    \"\"\"\n",
    "    X = pose_arr.copy()\n",
    "    T = X.shape[0]\n",
    "    nonzero_idx = np.where(~np.all(X == 0, axis=1))[0]\n",
    "    if len(nonzero_idx) == 0:\n",
    "        return X\n",
    "\n",
    "    first, last = nonzero_idx[0], nonzero_idx[-1]\n",
    "    for t in range(0, first):\n",
    "        X[t] = X[first]\n",
    "    for t in range(last+1, T):\n",
    "        X[t] = X[last]\n",
    "\n",
    "    jumps = np.where(np.diff(nonzero_idx) > 1)[0]\n",
    "    for g in jumps:\n",
    "        a = nonzero_idx[g]\n",
    "        b = nonzero_idx[g+1]\n",
    "        gap = b - a - 1\n",
    "        if gap <= 0:\n",
    "            continue\n",
    "        if gap <= max_gap:\n",
    "            for k, t in enumerate(range(a+1, b), start=1):\n",
    "                alpha = k / (gap + 1)\n",
    "                X[t] = (1 - alpha) * X[a] + alpha * X[b]\n",
    "        else:\n",
    "            for t in range(a+1, b):\n",
    "                X[t] = X[a]\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# Face features (EffNet-B2) and audio (Wav2Vec2) — UNCHANGED\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "mtcnn = MTCNN(keep_all=False, min_face_size=20, device=device)\n",
    "\n",
    "def detect_face(frame_bgr):\n",
    "    \"\"\"Detect face -> 224x224 tensor, or None (UNCHANGED).\"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    boxes, _ = mtcnn.detect(frame_rgb)\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        x1, y1, x2, y2 = boxes[0].astype(int)\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        roi = frame_rgb[y1:y1+h, x1:x1+w]\n",
    "        if roi.size == 0:\n",
    "            return None\n",
    "        face_resized = cv2.resize(roi, (224, 224))\n",
    "        return torch.tensor(face_resized).permute(2, 0, 1).float() / 255.0\n",
    "    return None\n",
    "\n",
    "efficientnet = models.efficientnet_b2(weights=EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "efficientnet = torch.nn.Sequential(*list(efficientnet.children())[:-1])  # Remove final FC\n",
    "efficientnet.eval().to(device)\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-robust\",\n",
    "    return_attention_mask=True\n",
    ")\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-robust\",\n",
    "    use_safetensors=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "wav2vec_model.eval().to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _safe_replace(src, dst):\n",
    "    \"\"\"Move with overwrite (Windows-safe).\"\"\"\n",
    "    if os.path.exists(dst):\n",
    "        os.remove(dst)\n",
    "    os.replace(src, dst)\n",
    "\n",
    "# ----------------------------\n",
    "# One video\n",
    "# ----------------------------\n",
    "def process_single_video(video_path, audio_dir, output_dir, temp_dir):\n",
    "    \"\"\"Process one video (A/V/P) and overwrite outputs.\"\"\"\n",
    "    sample_id = os.path.basename(video_path).replace('.flv', '')\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        emotion = sample_id.split(\"_\")[2]\n",
    "        label = EMOTION_MAP[emotion]\n",
    "        logging.info(f\"Processing {sample_id} (emotion: {emotion})\")\n",
    "\n",
    "        # -------------- read frames (uniformly sample to 90) --------------\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            logging.error(f\"Could not open video: {video_path}\")\n",
    "            return False\n",
    "\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "        if total > 0:\n",
    "            idx_keep = np.linspace(0, total - 1, TARGET_VIDEO_FRAMES, dtype=int)\n",
    "            want = set(idx_keep.tolist())\n",
    "        else:\n",
    "            want = None  # fallback: take first 90 frames\n",
    "\n",
    "        video_feats, pose_vecs = [], []\n",
    "        i = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if want is not None and i not in want:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # ---- visual features (UNCHANGED) ----\n",
    "            face_t = detect_face(frame)\n",
    "            if face_t is not None:\n",
    "                with torch.no_grad(), torch.amp.autocast(\n",
    "                    'cuda' if device.type=='cuda' else 'cpu', enabled=(device.type=='cuda')\n",
    "                ):\n",
    "                    f = efficientnet(face_t.unsqueeze(0).to(device)).squeeze()\n",
    "                video_feats.append(f.cpu().numpy())\n",
    "            else:\n",
    "                video_feats.append(np.zeros(1408, dtype=np.float32))\n",
    "\n",
    "            # ---- pose vec (UPDATED) ----\n",
    "            vec156 = extract_pose_features(frame)\n",
    "            pose_vecs.append(vec156.astype(np.float32))\n",
    "\n",
    "            i += 1\n",
    "            if len(pose_vecs) >= TARGET_VIDEO_FRAMES:\n",
    "                break\n",
    "        cap.release()\n",
    "\n",
    "        # pad if needed\n",
    "        while len(video_feats) < TARGET_VIDEO_FRAMES:\n",
    "            video_feats.append(np.zeros(1408, dtype=np.float32))\n",
    "        while len(pose_vecs) < TARGET_VIDEO_FRAMES:\n",
    "            pose_vecs.append(np.zeros(156, dtype=np.float32))\n",
    "\n",
    "        video_np = np.stack(video_feats).astype(np.float32)      # (90, 1408)\n",
    "        pose_np_raw = np.stack(pose_vecs).astype(np.float32)     # (90, 156)\n",
    "        pose_np = fill_pose_gaps(pose_np_raw, max_gap=8)         # gap-filled\n",
    "\n",
    "        # ---- audio (UNCHANGED) ----\n",
    "        audio_path = os.path.join(audio_dir, sample_id)\n",
    "        if not os.path.exists(audio_path):\n",
    "            found = False\n",
    "            for ext in ['', '.wav', '.WAV', '.wav.wav']:\n",
    "                alt = os.path.join(audio_dir, f\"{sample_id}{ext}\")\n",
    "                if os.path.exists(alt):\n",
    "                    audio_path = alt\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                logging.error(f\"Audio file missing for {sample_id}\")\n",
    "                return False\n",
    "\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        waveform = waveform.squeeze()\n",
    "        if sr != 16000:\n",
    "            waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "        waveform = waveform[:16000 * 3]  # 3 seconds\n",
    "\n",
    "        inputs = feature_extractor(\n",
    "            waveform, sampling_rate=16000, return_tensors=\"pt\",\n",
    "            padding=\"max_length\", max_length=16000*3, truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad(), torch.amp.autocast(\n",
    "            'cuda' if device.type=='cuda' else 'cpu', enabled=(device.type=='cuda')\n",
    "        ):\n",
    "            out = wav2vec_model(\n",
    "                input_values=inputs[\"input_values\"], attention_mask=inputs.get(\"attention_mask\", None)\n",
    "            )\n",
    "            audio_features = out.last_hidden_state.squeeze().cpu().float().numpy()  # (T,1024)\n",
    "\n",
    "        if len(audio_features) > TARGET_AUDIO_FRAMES:\n",
    "            audio_np = audio_features[:TARGET_AUDIO_FRAMES]\n",
    "        else:\n",
    "            pad = TARGET_AUDIO_FRAMES - len(audio_features)\n",
    "            audio_np = np.vstack([audio_features, np.zeros((pad, 1024), dtype=np.float32)])\n",
    "\n",
    "        # ---------------- save (atomic overwrite from temp) ----------------\n",
    "        tmp_paths = []\n",
    "        tmp_paths.append(os.path.join(TEMP_DIR, f\"{sample_id}_label.npy\"))\n",
    "        np.save(tmp_paths[-1], np.array([label], dtype=np.int64))\n",
    "\n",
    "        tmp_paths.append(os.path.join(TEMP_DIR, f\"{sample_id}_video_frames.npy\"))\n",
    "        np.save(tmp_paths[-1], video_np)\n",
    "\n",
    "        tmp_paths.append(os.path.join(TEMP_DIR, f\"{sample_id}_audio_frames.npy\"))\n",
    "        np.save(tmp_paths[-1], audio_np)\n",
    "\n",
    "        tmp_paths.append(os.path.join(TEMP_DIR, f\"{sample_id}_pose.npy\"))\n",
    "        np.save(tmp_paths[-1], pose_np)\n",
    "\n",
    "        # move with overwrite\n",
    "        for src in tmp_paths:\n",
    "            dst = os.path.join(OUTPUT_DIR, os.path.basename(src))\n",
    "            _safe_replace(src, dst)\n",
    "\n",
    "        # quick stats to log\n",
    "        zero_raw = float(np.mean(np.all(pose_np_raw == 0, axis=1)))\n",
    "        zero_filled = float(np.mean(np.all(pose_np == 0, axis=1)))\n",
    "        logging.info(f\"{sample_id}: pose zero_rows raw={zero_raw:.3f} -> filled={zero_filled:.3f}\")\n",
    "        logging.info(f\"Processed {sample_id} in {time.time()-start_time:.2f}s\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Failed {sample_id}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# ----------------------------\n",
    "# Verify\n",
    "# ----------------------------\n",
    "def verify_processed_data(output_dir):\n",
    "    print(\"\\nVerifying processed data...\")\n",
    "    vids = [f.replace(\"_video_frames.npy\",\"\") for f in os.listdir(output_dir) if f.endswith(\"_video_frames.npy\")]\n",
    "    if not vids:\n",
    "        print(\"No video feature files found.\")\n",
    "        return False\n",
    "    ok = True\n",
    "    for sid in vids[:5]:\n",
    "        video = np.load(os.path.join(output_dir, f\"{sid}_video_frames.npy\"))\n",
    "        audio = np.load(os.path.join(output_dir, f\"{sid}_audio_frames.npy\"))\n",
    "        pose  = np.load(os.path.join(output_dir, f\"{sid}_pose.npy\"))\n",
    "\n",
    "        if video.shape != (TARGET_VIDEO_FRAMES, 1408):\n",
    "            ok = False; print(f\" ! video shape bad for {sid}: {video.shape}\")\n",
    "        if audio.shape != (TARGET_AUDIO_FRAMES, 1024):\n",
    "            ok = False; print(f\" ! audio shape bad for {sid}: {audio.shape}\")\n",
    "        if pose.shape != (TARGET_VIDEO_FRAMES, 156):\n",
    "            ok = False; print(f\" ! pose shape bad for {sid}: {pose.shape}\")\n",
    "\n",
    "        # pose zero rows should be ~0 after fill\n",
    "        if np.mean(np.all(pose == 0, axis=1)) > 0.01:\n",
    "            ok = False; print(f\" ! residual zero pose rows for {sid}\")\n",
    "\n",
    "    if ok:\n",
    "        print(\" All verification checks passed!\")\n",
    "    return ok\n",
    "\n",
    "# ----------------------------\n",
    "# Orchestrator\n",
    "# ----------------------------\n",
    "def process_all_videos(video_dir, audio_dir, output_dir, test_mode=False, max_test_samples=10):\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.lower().endswith(\".flv\")]\n",
    "    video_files.sort()\n",
    "    if test_mode:\n",
    "        video_files = video_files[:max_test_samples]\n",
    "\n",
    "    processed = 0\n",
    "    for vf in tqdm(video_files, desc=\"Processing videos (A/V/P)\"):\n",
    "        path = os.path.join(video_dir, vf)\n",
    "        if process_single_video(path, audio_dir, output_dir, TEMP_DIR):\n",
    "            processed += 1\n",
    "\n",
    "    print(f\"\\nDone. processed={processed}/{len(video_files)}\")\n",
    "    return processed\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*50)\n",
    "    print(\"TRIMODAL PREPROCESSING SCRIPT (A/V/P, overwrite)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    config = {\n",
    "        \"CREMA_D_ROOT\": CREMA_D_ROOT,\n",
    "        \"AUDIO_DIR\": AUDIO_DIR,\n",
    "        \"VIDEO_DIR\": VIDEO_DIR,\n",
    "        \"OUTPUT_DIR\": OUTPUT_DIR,\n",
    "        \"TARGET_VIDEO_FRAMES\": TARGET_VIDEO_FRAMES,\n",
    "        \"TARGET_AUDIO_FRAMES\": TARGET_AUDIO_FRAMES\n",
    "    }\n",
    "    with open(os.path.join(CREMA_D_ROOT, \"preprocessing_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    print(\"\\nConfiguration:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    # quick test first (10)\n",
    "    print(\"\\nRunning in TEST MODE (10 samples only)...\")\n",
    "    test_count = process_all_videos(VIDEO_DIR, AUDIO_DIR, OUTPUT_DIR, test_mode=True)\n",
    "    if test_count > 0:\n",
    "        print(\"\\nTest mode successful! Verifying test data...\")\n",
    "        if verify_processed_data(OUTPUT_DIR):\n",
    "            print(\"\\nTest data verified successfully!\")\n",
    "            print(\"Proceeding to full dataset processing...\")\n",
    "\n",
    "            # FULL RUN (all videos)\n",
    "            process_all_videos(VIDEO_DIR, AUDIO_DIR, OUTPUT_DIR, test_mode=False)\n",
    "\n",
    "            # Final verification\n",
    "            print(\"\\nVerifying full dataset...\")\n",
    "            verify_processed_data(OUTPUT_DIR)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"PREPROCESSING COMPLETE!\")\n",
    "            print(\"=\"*50)\n",
    "        else:\n",
    "            print(\"\\nTest data verification failed. Check logs for details.\")\n",
    "    else:\n",
    "        print(\"\\n Test mode failed. Check logs for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f62a6e",
   "metadata": {},
   "source": [
    "### Splitting Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = r\"E:\\Research_Datasets\\processed_features2\"\n",
    "SPLIT_DIR = r\"E:\\Research_Datasets\\processed_features_split2\"\n",
    "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
    "\n",
    "# Get all unique sample bases (e.g., \"1001_DFA_ANG_XX\")\n",
    "samples = set()\n",
    "for filename in os.listdir(PROCESSED_DIR):\n",
    "    if filename.endswith(\"_video_frames.npy\"):\n",
    "        base_name = filename.replace(\"_video_frames.npy\", \"\")\n",
    "        samples.add(base_name)\n",
    "\n",
    "samples = list(samples)\n",
    "print(f\"Found {len(samples)} unique samples\")\n",
    "\n",
    "# Create 5 different splits\n",
    "for split_num in range(1, 6):\n",
    "    print(f\"\\nCreating Split {split_num}...\")\n",
    "    split_path = os.path.join(SPLIT_DIR, f\"split_{split_num}\")\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "    # Shuffle a COPY so each split is independent\n",
    "    samples_shuffled = samples.copy()\n",
    "    np.random.seed(42 + split_num)\n",
    "    np.random.shuffle(samples_shuffled)\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    n = len(samples_shuffled)\n",
    "    train_samples = samples_shuffled[:int(0.7 * n)]\n",
    "    val_samples   = samples_shuffled[int(0.7 * n):int(0.85 * n)]\n",
    "    test_samples  = samples_shuffled[int(0.85 * n):]\n",
    "\n",
    "    # Create subset folders\n",
    "    for subset in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(split_path, subset), exist_ok=True)\n",
    "\n",
    "    # Copy files for each subset\n",
    "    for base_name in samples_shuffled:\n",
    "        for subset, subset_samples in ((\"train\", train_samples),\n",
    "                                       (\"val\",   val_samples),\n",
    "                                       (\"test\",  test_samples)):\n",
    "            if base_name in subset_samples:\n",
    "                for modality in [\"video_frames\", \"audio_frames\", \"pose\", \"label\"]:\n",
    "                    src_file = os.path.join(PROCESSED_DIR, f\"{base_name}_{modality}.npy\")\n",
    "                    dst_file = os.path.join(split_path, subset, f\"{base_name}_{modality}.npy\")\n",
    "                    if os.path.exists(src_file):\n",
    "                        shutil.copy(src_file, dst_file)\n",
    "                    else:\n",
    "                        print(f\" Missing file: {src_file}\")\n",
    "\n",
    "    # Verify no sample leakage\n",
    "    def bases_in(sub):\n",
    "        p = os.path.join(split_path, sub)\n",
    "        return {f.replace(\"_video_frames.npy\", \"\")\n",
    "                for f in os.listdir(p) if f.endswith(\"_video_frames.npy\")}\n",
    "    train_files = bases_in(\"train\")\n",
    "    val_files   = bases_in(\"val\")\n",
    "    test_files  = bases_in(\"test\")\n",
    "\n",
    "    assert train_files.isdisjoint(val_files), \"Sample leakage between train and val\"\n",
    "    assert train_files.isdisjoint(test_files), \"Sample leakage between train and test\"\n",
    "    assert val_files.isdisjoint(test_files),   \"Sample leakage between val and test\"\n",
    "    print(f\"Split {split_num}: No sample leakage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6790b95",
   "metadata": {},
   "source": [
    "### Checking Class Balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def verify_splits(split_dir):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SPLIT VERIFICATION CONFIRMATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for split_num in range(1, 6):\n",
    "        split_path = os.path.join(split_dir, f\"split_{split_num}\")\n",
    "        if not os.path.isdir(split_path):\n",
    "            print(f\"Split {split_num}: folder not found -> {split_path}\")\n",
    "            continue\n",
    "\n",
    "        issues = []\n",
    "\n",
    "        def get_samples_from_video(subset):\n",
    "            folder = os.path.join(split_path, subset)\n",
    "            if not os.path.isdir(folder):\n",
    "                return set()\n",
    "            return {\n",
    "                f.replace(\"_video_frames.npy\", \"\")\n",
    "                for f in os.listdir(folder)\n",
    "                if f.endswith(\"_video_frames.npy\")\n",
    "            }\n",
    "\n",
    "        train_samples = get_samples_from_video(\"train\")\n",
    "        val_samples   = get_samples_from_video(\"val\")\n",
    "        test_samples  = get_samples_from_video(\"test\")\n",
    "\n",
    "        # leakage checks\n",
    "        if train_samples & val_samples:\n",
    "            issues.append(f\"Train-Val leakage: {len(train_samples & val_samples)} samples\")\n",
    "        if train_samples & test_samples:\n",
    "            issues.append(f\"Train-Test leakage: {len(train_samples & test_samples)} samples\")\n",
    "        if val_samples & test_samples:\n",
    "            issues.append(f\"Val-Test leakage: {len(val_samples & test_samples)} samples\")\n",
    "\n",
    "        # file presence check (ALL must be .npy)\n",
    "        for subset, samples in [(\"train\", train_samples),\n",
    "                                (\"val\",   val_samples),\n",
    "                                (\"test\",  test_samples)]:\n",
    "            folder = os.path.join(split_path, subset)\n",
    "            for sample in samples:\n",
    "                required_files = [\n",
    "                    f\"{sample}_video_frames.npy\",\n",
    "                    f\"{sample}_audio_frames.npy\",\n",
    "                    f\"{sample}_pose.npy\",\n",
    "                    f\"{sample}_label.npy\"\n",
    "                ]\n",
    "                missing = [f for f in required_files\n",
    "                           if not os.path.exists(os.path.join(folder, f))]\n",
    "                if missing:\n",
    "                    issues.append(f\"Missing files for {sample} in {subset}: {missing}\")\n",
    "\n",
    "        print(f\"Split {split_num} sizes — train:{len(train_samples)}  val:{len(val_samples)}  test:{len(test_samples)}\")\n",
    "        if not issues:\n",
    "            print(f\"Split {split_num}:  Perfect — no leakage, files complete\")\n",
    "        else:\n",
    "            print(f\"Split {split_num}:  Issues found:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  - {issue}\")\n",
    "\n",
    "\n",
    "verify_splits(r'E:\\Research_Datasets\\processed_features_split2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
